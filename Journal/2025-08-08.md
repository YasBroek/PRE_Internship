# 2025-08-08

- Finished correcting Literature Review and Mathodology parts on report
    - Decided not to modify results and discussion yet as we're still doing some tests
- Ran more tests 
    - Created a simple instance map with two scenarios
        1: PP isn't originally able to find a path
            - In this case, loss graphic stays noisy and doesn't converge
        2: PP gives a solution worse than what we can think of
            - In this case, loss converges, but doesn't generalize to other scenarios
            - resulting weights don't necessarily make sense
    - Also tested it with "room-32-32-4", as we'd found that the previous problem had to do with InferOpt.jl
        - Trained with just 1 configuration
            - Overfitting after about 120 epochs
            - Even at its lower loss, it doesnt't generalize (applying learnt weights to other instances actually worsened PP cost)
        - Trained with 25 configurations
            - Still doesn't generalize
- Reflections:
    - Situations where PP originally doesn't find a path
        - Not sure to what point learning weights is capable of helping, as there are some situations in which some edges are the only way for agents to get to their goals, and in PP the plan their paths without thinking of other agents
    - Another thing I started thinking about is to what point weighting weights will actually help for planning with PP? Because every scenario can present an entirelly different situation, and as previously planned agents behave as obstacles for others, isn't it as if we're changing the graph? Do weights predicted for a nomber of situations actually represent what's happening in the oher one?