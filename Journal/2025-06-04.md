# 2025-06-04

- Trying to materialize ideas for application of gradient descent on ML prediction of parameters
    - Analytical Smoothing of Optimization Mappings: Examples showed in the articles concern mainly LP problems -> may be applicable, but not ideal: would have to use algorithms with Linear Programming (not the case of PP, for example)
    - Smoothing by random perturbations: apply perturbation to weights with noise (using normal distribution) and run PP with the perturbed weights, approximate loss using perturbations of parameters and update w using gradient descent
        - We would need to define and tune noise variance and learning rate
    - Differentiation of Surrogate Loss Functions: we could try applying a SPO+. We could define our loss as -f(theta) (theta being the predicted weights), and use the expression for SPO+ from the paper, with c having all weights equal to one and c^ the predicted weights

- Coding
    - Improved visualization and made it a function
    - Adapted the code so I can manipulate number of agents in instance
    - Implemented a_star for independent shortest path
    - Changed grath type into weighted graph (figured it'll be usefull in the future)
    - Created a function to identify conflict positions between agents
    - Adapted paths list (now it contains lists of edges through which each agent passes)