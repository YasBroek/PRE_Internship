# 2025-06-04

- Trying to materialize ideas for application of gradient descent on ML prediction of parameters
    - Analytical Smoothing of Optimization Mappings: Examples showed in the articles concern mainly LP problems -> may be applicable, but not ideal: would have to use algorithms with Linear Programming (not the case of PP, for example)
    - Smoothing by random perturbations: apply perturbation to weights with noise (using normal distribution) and run PP with the perturbed weights, approximate $ \frac{\partial L}{\partial \mathbf{x}_\sigma} $ using perturbations of $ {\mathbf{x}_\sigma} $ and update w using gradient descent
        - We would need to define and tune noise variance and learning rate
    - Differentiation of Surrogate Loss Functions: we could try applying a SPO+. We could define our loss as -f($\theta$) ($\theta$ being the predicted weights), and use the expression $ L_{\text{SPO+}}(x^*(\hat{c})) = 2\hat{c}^\top x^*(c) - c^\top x^*(c) + \max_{x \in F} \{ c^\top x - 2\hat{c}^\top x \} $, keeping in mind that $ f(\mathbf{x}, \mathbf{c}) = \mathbf{c}^\top \mathbf{x} $, $ c^\top x^*(c) $ a vector of true weights (all equal to 1), $ \hat{c}^\top x^*(c) $ predicted weights