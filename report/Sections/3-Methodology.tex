\section{Methodology}

The pipeline of this study consists of elaborating a Machine Learning model capable of predicting imaginary edge weights to guide the path-finding algorithms into computing better solutions, with lower overall path costs.

For that, we construct a ML algorithm $\phi_w$ to predict modified edge weights $\theta$, which will then be used by a path-finding algorithm to compute a solution path. The representation of the pipeline is represented by Figure \ref{fig:pipeline-draft}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Images/pipeline draft.png}
    \caption{Caption}
    \label{fig:pipeline-draft}
\end{figure}

The training procedure is based on minimizing the final cost of the optimization objective, using a supervised learning method, learning from imitation of already existing best found solutions to determined instances.

The use of a ML model based on Gradient Descent, with a loss aimed directly at optimizing the objective function of the constrained Integer Linear Program that represents MAPF, is something that hasn't yet been tested in the MAPF literature, thus constituting the pipeline of this present study.

\subsection{PathFinding Algorithms}
Several pathfinding algorithms were elaborated by earlier studies as alternatives for solutions to the MAPF problem, as mentioned before. The ones that based this study of Decision-Focused learning application for Multi-Agent Path Finding were mainly two: Independent Shortest Paths and Prioritized Planning.

\subsubsection{Independent Shortest Paths}
In this work, we generate initial paths for all agents using a function that computes the shortest path for each agent individually, based on Dijkstraâ€™s algorithm, without incorporating any constraints. As a result, the initial paths may contain spatial or temporal conflicts, such as agents occupying the same node at the same time or moving along the same edge in opposite directions.

While these paths do not form a truly independent solution in the MAPF sense, they serve as a useful baseline for further processing.

This step also allows us to analyze the structure of the problem and identify common conflict patterns, which can later be used to design cost functions or guide learning.

\subsubsection{Prioritized Planning}

The Prioritized Planning approach is designed to calculate actual paths and their associated costs after training by ordering agents according to a pre-determined heuristic before path calculation.

In the developed code, agents are sorted based on a Start-and-Goal Conflict heuristic, inspired by \cite{buckleyFastMotionPlanning1989}. The heuristic elaborated downplays agents whose goal positions are more likely to overlap with other agents' paths (which is determined using ISP), forcing these agents to be planned last.

Planning proceeds sequentially: the highest-priority agent retains its shortest path, while each subsequent agent plans its path to avoid space-time conflicts with all higher-priority agents. This is achieved by treating the already planned paths as dynamic obstacles within a space-time representation of the environment.

Several attempts were made to implement a function capable of executing the Prioritized Planning (PP) algorithm. One attempt, for example, involved planning a path for each agent individually and introducing an obstacle for the current agent whenever a conflict with a previously planned path was detected. In this context, introducing an obstacle meant that the agent would treat the conflicting cell as if it were a static obstacle on the original map. However, this approach was not pursued further, as it did not faithfully capture the fundamental principles of the PP algorithm. In Prioritized Planning, conflicts between agents are temporal, restricted to specific time steps, whereas this method imposed permanent spatial constraints. As a result, it led to unnecessarily restrictive planning and deviated from the intended temporal coordination that characterizes PP.

The most accurate and effective function developed utilized a \textbf{Time-Expanded Graph (TEG)} structure, as illustrated in Figure \ref{TEG}. This structure represents a specialized type of graph that incorporates the dimension of time, consisting of the original graph's vertices for each time step, with edges representing the possible movements from a vertex at time $ t $ to a vertex at time $ t+1 $.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Images/TEG.png}
    \caption{Time-Expanded Graph construction}
    \label{TEG}
\end{figure}

For this new graph representation, vertex index conversion is achieved through the formula:

\[ v_{TEG} = (t-1) \times n + v_{original} \tag{10} \]

where $ v_{TEG} $ denotes the vertex index on the Time-Expanded Graph, $ t $ is the current time step, $ n $ is the total number of vertices in the original graph, and $ v_{original} $ is the corresponding vertex from the original graph.

In our implementation, Time-Expanded Graphs also accept lists of vertices and edges to be removed from the graph at specific time steps, which are considered when constructing the edges. This allows for the removal of only those actions that would lead to conflicts with agents whose paths have already been planned, thereby avoiding unnecessary vertex and edge removals across all time steps, removing only the edges and vertices that generated conflicts at their specific time steps, and enabling greater possibilities for future agents.

With the introduction of this new graph type, the function for executing Prioritized Planning initially orders agents hierarchically and creates an initial TEG with an arbitrary duration $ T $ (sufficiently long to prevent unnecessary calculations, yet not excessively so). It then calculates paths for each agent, adding all new edges and vertices from those paths into the removal lists, adapting the TEG accordingly, and continues this process until all agents are planned. If an agent reaches its destination before $ T $, it remains there until the end; if $ T $ is insufficient for an agent to reach its target, it is extended, and all previously planned agents remain at their positions until the end.

This prioritized re-planning approach effectively eliminates collisions and offers a more feasible solution set compared to naive, uncoordinated planning. Furthermore, the process is computationally efficient and scalable for moderate agent counts (up to 30 agents on maps constituting grids with a maximum size of 128x128 cells).

While the current implementation employs a fixed heuristic ordering, this structure is compatible with future enhancements, such as learned priority orderings, as proposed in recent research \cite{zhangLearningPriorityOrdering2022}. These could be integrated into a differentiable pipeline or through offline training on representative MAPF instances.

The technical specifics of the ordering heuristic and re-planning mechanism are detailed in Annex Y.

\subsection{Training Procedure}

\subsubsection{Training Approaches}
Two different approaches for constructing the training model were explored in this study:
\begin{enumerate}
    \item \textbf{Learning through feature attribution}:  In this approach, a set of hand-crafted features was defined for each edge in a MAPF instance graph. The features were encoded in a matrix representation and used as input to train a model to predict a weight for each edge based on its attributed features. This method provides a structured way to incorporate domain knowledge into the learning process and offers interpretability, as it enables the analysis of how individual features influence the resulting edge weights.
    \item \textbf{Learning from map: } The second approach aimed to train a model to learn edge weights directly from the raw map representation, without relying on predefined features. This allows the model to capture spatial patterns and interactions implicitly, leading to less generalizable behavior, as it's restrict to learning from a single map structure, and predicting weights specific to that map. 
    
    Nevertheless, it enables a direct evaluation of the impact of the learning process, as it facilitates the analysis of the final edge weights, and their comparison to what would be expected, based on a notion of the optimal paths (for simpler instances, we are able to tell if an edge should be avoided and therefore should be attributed a higher weight, and vice versa, which would be more difficult to tell from weighting different features).
\end{enumerate}
Both methods were developed using the same principles for training, their main difference being the feature attribution phase.

\subsubsection{Feature Attribution}
This step, present only on the first approach for training, consisted on defining a number of features for each edge on a MAPF instance graph, represented by a matrix $E \times F$, where $E$ is the number of edges in the graph and $F$ is the number of different features evaluated. 
The main features computed in the model were:
\begin{enumerate}
    \item \textbf{Tendence for conflicts:} Based on individual paths precomputed using ISP, this feature attributes higher values for edges in which most conflicts happen. It is computed by counting the number of times an edge suffers a conflict on the ISP paths, either when more than one agent pass through it at the same time, or when more than one agent reach its destine vertex at the simultaneously. Each conflict configure a value of 1, and the final value of the feature represents the sum of conflicts for each edge.
    \item \textbf{Degree:} Equal to the degree of the destination vertex;
    \item \textbf{Harmonic Centrality \cite{Centrality2025}:} Measures how close the destination vertex is to all other vertices in the graph. It gives higher values to nodes that can reach others more easily. In our model, each edge gets the centrality value of its destination node, so edges leading to more "central" areas in the graph are given more importance.
    \item \textbf{Most used edges:} Also based on ISP precomputed paths, this feature is proportional to the number of times agents pass through it;
    \item \textbf{Distance to closest obstacle:} Corresponds to distance from edge's destine vertex to its closest obstacle;
    \item \textbf{Distance to all agents:} Proportional to the sum of distances from all agent goal spots to the edge's destine vertex;
    \item \textbf{Distance to closest agent:} Measures distance from edge's destine vertex to closest agent goal;
    \item \textbf{Number of agents close:} Counts number of agent goals that are located at a predetermined radius (in this context we assumed $\sqrt{w}$, $w$ being the width of the instance map) from edge's destine vertex.
\end{enumerate}
All the computed features were then normalized (by taking the highest value of each feature and dividing every other value by it) to be treated afterwards by the training model.

\subsubsection{Linear Regression application}

The training model for this study was initially elaborated manually, and its pseudo-code can be consulted on Appendix XXXX. It takes as inputs a list of problem instances \cite{shenTrackingProgressMultiAgent2023}, containing different scenarios, and a list containing their known optimal solutions, taken from \cite{shenTrackingProgressMultiAgent2023}, which contains all progress and best solutions found to this day for different MAPF problem instances.

The training process consists of computing the features matrix for each of the instances on the input list, and then letting the model learn the weights for each feature of each edge, through Linear Regression ($c = Xw$, where $X$ and $w$ stand for the features matrix and the predicted weights matrix, respectively), by iterating through each instance on the list, for every epoch.

The optimization algorithm used for this learning model was Independent Shortest Paths, compatible with the method being applied, as it is differentiable.

To make Gradient Descent on weight prediction possible, Monte Carlo perturbations \cite{berthetLearningDifferentiablePertubed2020} were also applied to the weight vector as the algorithm calculated $y^*_\epsilon(\hat\theta)$ from Equation 1, allowing Fenchel Young Loss computation. 

To make the use of the original solution outputs (paths $P=\{P_1, P_2,...,P_k\}$ constituted by lists of either graphs edges $E=\{e_1,e_2, ..., e_m\}$ or vertices $V=\{v_1, v_2,...,v_n\}$) compatible for learning equations, they where transformed into vectors using:

\[
\sum^k_{i=1}\sum_{e \in P_i}\delta_{e_j}(e), \text{  for  } j = 1,2,...,m \tag{11}
\]

where

\[
\delta_{e_j}(e) = 
\begin{cases}
    1 & \text{if } e \equiv e_j \\
    0 & \text{else}
\end{cases}
\]

So the new path vectors actually represented the number of times they were passed through while agents executed their respective paths.

Equations (??) and (??) were used for calculating the loss and its gradient, and weights matrix was updaded through the following equation:

\[
w = w - \alpha \times f \times \nabla_w \mathcal{L}_\varepsilon(w; y) \tag{12}
\],

$\alpha$ representing the learning rate and $f$ the features matrix.

\subsubsection{Generalized linear model}

In this study, an alternative approach to directly applying standard linear regression for learning weights was explored: the constitution of a generalized linear model. This involved repeating the overall structure of the training model but replacing the traditional linear regression computation with various non-linear transformations applied to the predicted weights, $\hat{\theta}$. The primary motivation for these alternative transformations was to avoid negative weights, which are not supported by Dijkstra's algorithm for shortest path calculations (as edge costs must be non-negative for its proper functioning).

Specifically, the following variations were investigated to ensure non-negative or appropriately scaled weights:

\begin{enumerate}
    \item \textbf{abs.:} This transformation directly takes the absolute value of the predicted weights.
    \[\hat{\theta} = |Xw| \tag{13}\]
    \item \textbf{$\sigma$:} The sigmoid function squashes the raw linear output into a range between 0 and 1. This can be useful if the weights are intended to represent probabilities or normalized values.
    \[\hat{\theta} = \frac{1}{1+e^{-Xw}} \tag{14}\]
    \item \textbf{Rectified Linear Unit (ReLU):} ReLU sets any negative input to zero, effectively ensuring non-negative weights. It is computationally efficient and widely used in neural networks.
    \[\hat{\theta} = max(0,Xw) \tag{15}\]
    \item \textbf{softplus:} The softplus function is a smooth approximation of the ReLU function. It ensures positive outputs while maintaining differentiability across its domain, which can be beneficial for gradient-based optimization.
    \[\hat{\theta} = \ln(1+e^{Xw}) \tag{16}\]
    \item \textbf{Projected Gradient Descent (PGD):} Instead of applying a fixed non-linear transformation, this approach incorporates the non-negativity constraint directly into the optimization process. After each gradient descent step, the weights are projected back onto the feasible set (i.e., non-negative values). This is typically done by setting any negative weights to zero. 
    \[ \hat{\theta}_{k+1} = \text{Projection}_{\mathbb{R}_{\ge 0}}(\hat{\theta}_k - \alpha \nabla L(\hat{\theta}_k)) \tag{17} \] 
    where $\alpha$ is the learning rate and $\text{Projection}_{\mathbb{R}{\ge 0}}(x) = \max(0, x)$. 
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/x_original.png}
        \caption{$f(x) = x$}
        \label{fig:x_original}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/abs_x.png}
        \caption{$abs(x) = |x|$}
        \label{fig:abs_x}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/sigmoid_x.png}
        \caption{$\sigma(x) = \frac{1}{1+e^{-x}}$}
        \label{fig:sigmoid_x}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/relu_x.png}
        \caption{$ReLU(x) = max(0, x)$}
        \label{fig:relu_x}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/softplus_x.png}
        \caption{$softplus(x) = ln(1+e^x)$}
        \label{fig:softplus_x}
    \end{subfigure}
    
    \caption{Variations of functions used for Generalized Model applied over $f(x) = x$.}
    \label{fig:GLM}
\end{figure}
These alternative transformations help the model to avoid negative weights, which are not supported by Dijkstra's Independent Paths calculation.

\subsubsection{Testing}

After training the model with a list of instances, it was necessary to test it with a different set of test instances to evaluate the training process, and to know if it's actually better than implementing no training process at all. 

For us to be able to get to those conclusions, we applied the Prioritized Planning algorithm to compute paths for two different versions of each instance in a test set: the original version and another one for which the learned weights are applied (only for path computation, once the calculation of path costs always takes into account original weights). We compare the path costs obtained for the different versions to conclude on training efficiency.

In the learning pipeline, we use Independent Shortest Paths (ISP) instead of Prioritized Planning due to the requirements of gradient-based optimization. Gradient descent relies on differentiability to update the learned weights based on how they influence the final loss. However, Prioritized Planning is a discrete, non-differentiable algorithm that introduces abrupt changes in agent paths based on planning order and conflict resolution, making it incompatible with gradient descent. In contrast, ISP computes paths independently for each agent based solely on the current weights, resulting in smoother, more stable outputs with respect to small weight changes. This allows it to serve as a differentiable approximation of the planning process during training. Once the weights are learned, we evaluate their effectiveness by applying them to Prioritized Planning, which provides more realistic, conflict-free paths for assessing overall performance.

\subsection{Julia libraries}

As the complexity of the model and its challenges developed, we opted to implement Julia packages to facilitate learning implementation. \textbf{Flux.jl} \cite{Flux.jl-2018} was used for the definition of the training model, \textbf{InferOpt.jl} \cite{dalleLearningCombinatorialOptimization2022} for loss computation and \textbf{Optimizers.jl} \cite{optimisersjl} for Gradient Descent calculation.