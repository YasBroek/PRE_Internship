\section{Discussion}

The results obtained from the experiments show that the use of the training methods that were proposed when computing paths actually worsened the optimization of the model, instead of helping the Prioritized Planning process.

Considering this result, different hypothesis are raised to explain the behavior observed, which will be elaborated in the following subsections:
\begin{enumerate}
\item \textbf{Insufficient training data}: The dataset may not have been large enough to capture the complexity of the problem.
\item \textbf{Inappropriate training data}: The data used for learning may not have been representative of the relevant features or decision boundaries.
\item \textbf{Lack of scenario diversity}: The training process may have lacked exposure to a sufficiently wide range of realistic scenarios.
\item \textbf{Mismatch between training and testing algorithms}: Significant differences between the algorithms used during training and those used during evaluation may have hindered generalization.
\end{enumerate}

\subsection{Insufficient training data}

The first hypothesis to explain the results obtained from training is that not enough training data was provided for learning in the context of a rather complex problem.

To experiment on that hypothesis, the training model (from learning directly from map) was applied with a total of 1025 instance scenarios.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/grad1025.png}
    \caption{Gradient of loss over time}
    \label{fig:grad_loss_1025}
\end{subfigure}
\hspace{0.01\textwidth}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/loss1025.png}
    \caption{Loss over time}
    \label{fig:loss_1025}
\end{subfigure}

\vspace{0.5cm}

\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/train1025.png}
    \caption{Training cost over time}
    \label{fig:training_cost_1025}
\end{subfigure}
\hspace{0.01\textwidth}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/test1025.png}
    \caption{Test cost over time}
    \label{fig:test_cost_1025}
\end{subfigure}
    
\caption{Statistics of learning evolution according to the number of epochs - training with 1025 instances}
\label{fig:loss_graph_1025}
\end{figure}

Analyzing the graphs presented in Figure \ref{fig:loss_graph_1025}, it is evident that the gradient stabilizes significantly faster compared to previous tests (Figure \ref{fig:loss_graph}), which were conducted with only 25 scenarios. This behavior is expected, as the current setup allows the model to iterate over more than 1000 additional scenarios within the same number of epochs. The loss follows a similar pattern as before, decreasing linearly and indicating that the model is effectively learning.

However, the training cost increases until it plateaus at a certain value and does not decrease further. Additionally, the test cost remains unstable even after 200 epochs, showing no clear tendency to decrease.

The analysis of training behavior even with a large training set indicates that insufficient training data probably isn't the main issue with the learning model outputs, leaving space for the development of alternative hypothesis.

\subsection{Inappropriate training data}

A second hypothesis that was raised referred to the qualitative aspect of the data, questioning the appropriateness of the scenarios provided to the model.

*****

\subsection{Lack of scenario diversity}

\subsection{Mismatch between training and testing algorithms}

Finally, another potential problem that might have impacted the learning process' application results is the use of different algorithms during the learning and testing phases: The training process applies Independent Shortest Paths to learn edge weights, while Prioritized Planning is applied when computing the final paths for agents.

The use of different algorithms is due to the non-differentiability of PP, rendering it inadequate for Gradient Descent, leaving ISP as a substitute for training.

To experiment on that hypothesis, the ML model was trained and tested on scenarios which originally contained no conflicts, that is, instances in which agent's paths didn't collide even when computed using ISP, making the ISP solution the most similar possible to the PP solution. The results are presented in \ref{fig:loss_graph_h}.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/grad_h.png}
    \caption{Gradient of loss over time}
    \label{fig:grad_loss_h}
\end{subfigure}
\hspace{0.01\textwidth}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/loss_h.png}
    \caption{Loss over time}
    \label{fig:loss_h}
\end{subfigure}

\vspace{0.5cm}

\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/train_h.png}
    \caption{Training cost over time}
    \label{fig:training_cost_h}
\end{subfigure}
\hspace{0.01\textwidth}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/test_h.png}
    \caption{Test cost over time}
    \label{fig:test_cost_h}
\end{subfigure}
    
\caption{Statistics of learning evolution according to the number of epochs - instances with no conflicts}
\label{fig:loss_graph_h}
\end{figure}

The Figure shows that, although the training and testing algorithms are at their most similar, training and test cost curves still tend to grow with training, indicating that might not be the actual problem with the studied method.