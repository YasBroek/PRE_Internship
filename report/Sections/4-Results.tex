\section{Results}

In our results section, we focus on analyzing the training impact over the overall calculated cost. That impact is measured, as mentioned before, by comparing path costs between the path that takes into account the learned model when computing Prioritized Planning, and the path that doesn't. In this section, when we mention "Cost Ratio" we're referring to the value obtained by

\[
\text{Cost Ratio} = \frac{\text{Path Cost (Training model + PP)}}{\text{Path Cost (PP)}} \tag{18}
\]

\subsection{Learning through feature attribution}

Table \ref{features_test} provides a comprehensive analysis of how the mean Cost Ratio varies with the number of distinct scenarios included in the training set, across different total numbers of agents (denoted as $m$). These results were derived from evaluating the trained model using the parameter configuration outlined in Table \ref{tab:features_parameters}. The number of training epochs was adjusted based on the number of scenarios, ensuring that the final loss remained sufficiently low.

The training set comprised randomly selected scenarios, which included both random and evenly distributed instances, with varying quantities of agents. All scenarios were related to the same map, "room-32-32-4", which was arbitrarily chosen from the "Tracking Progress in MAPF" website proposed by \cite{shenTrackingProgressMultiAgent2023}. The evaluation was conducted on a test set containing a variety of random scenarios, all associated with the same map used during training.

\begin{table}[!ht]
\begin{center}
\caption{Cost Ratio obtained by training from feature extraction for 10, 25 and 60 agents}
\label{features_test}
\begin{tabular}{||c c c c c||}  
  \hline 
  Number of scenarios & Final loss & m = 10 & m = 25 & m = 60 \\ [0.5ex]  
  \hline\hline 
  25 & $6.16 \times 10^{-3}$ & 1.0664 & 1.2035 & 1.4684  \\ 
  \hline
  50 & $7.24 \times10^{-4}$ & 1.1200 & 1.2999 & 1.5699  \\  
  \hline 
  100 & $1.28 \times 10^{-4}$ & 1.1356 & 1.3008 & 1.7181  \\ 
  \hline
  150 & $4.17 \times 10^{-6}$ & 1.2321 & 1.3541 & 1.7880 \\
  \hline 
  200 & $5.02 \times 10^{-6}$ & 1.2412 & 1.4317 & 1.6962 \\ [1ex]  
  \hline 
\end{tabular}
\end{center}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Experimental parameters for Learning through feature attribution}
    \begin{tabular}{|c|c|}
    \hline
        $\epsilon$ (Perturbation argument) & 0.001 \\
        $M$ (Iterations for Monte Carlos sampling) & 10 \\
        $\alpha$ (Learning rate) & 0.1 \\
    \hline
    \end{tabular}
    \label{tab:features_parameters}
\end{table}

The analysis of the results presented in Table \ref{features_test} indicates that supplying the trained model's predictions to the Prioritized Planning algorithm led to an increase in the total cost of the problem. Notably, there is a discernible trend showing that the Cost Ratio tends to rise with the number of scenarios. This suggests that, as the model gains more experience and learns from additional scenarios, the path cost increases correspondingly.

Figure \ref{fig:graphic_features} also shows the behavior of the Cost Ratio according to the number of agents considered when testing.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/impact_vs_agents_features.png}
    \caption{Cost Ratio on test set according to total number of agents}
    \label{fig:graphic_features}
\end{figure}

The Figure shows that the learned model's impact over the final path cost of the problem increases with the number of agents. This behavior is to be expected, as higher numbers of agents are associated with more conflict occurrences, which are taken into account when building the features matrix.

\subsection{Learning directly from map}

The different options of Generalized Linear Model functions were tested with predetermined values for the parameters and their results are shown in Table \ref{tab:GLM_performances}.
The tests were run preferentially with higher numbers of agents, as the impact of conflict potential is higher for this learning model.

\begin{table}[!ht]
\begin{center}
\caption{Cost Ratio results obtained for each Generalized Linear Model function}
\label{tab:GLM_performances}
\begin{tabular}{||c c c c||}  
  \hline 
  Linear model & Final loss & Cost Ratio \\ [0.5ex]  
  \hline\hline 
  abs & -11732.58 & 2.0366 \\  
  \hline
  Softplus & -9725.96 & 1.3700  \\  
  \hline 
  $\sigma$ & -51.91 & 1.0000  \\ 
  \hline 
  PGD & -11351.581 & 1.6703  \\ 
  \hline  
\end{tabular}
\end{center}
\end{table}

Table \ref{tab:GLM_performances} shows that the lower Cost Ratio was obtained with Softplus, although it is still higher than 1. $\sigma$ presented a strange behavior, as the analysis of output results showed that the function didn't actually learn anything, so the final path cost rested unchanged.

Different sets of input parameters for the training model were also tested and are presented in table \ref{tab:params}.

\begin{table}[!ht]
    \centering
    \caption{Input parameters tested}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Parameter & Values tested & Train cost ratio & Test cost ratio & (Train + Test) cost ratio \\
        \hline
        $\epsilon$ & \makecell{0.1 \\ \textbf{0.01} \\ 0.001} & \makecell{0.968 \\ \textbf{0.968} \\ 0.974} & \makecell{0.992 \\ \textbf{0.999} \\ 0.999} & \makecell{1.959 \\ \textbf{1.967} \\ 1.973} \\
        \hline
        $\alpha$ & \makecell{0.1 \\ 0.01 \\ 0.001 \\ \textbf{0.0001}} & \makecell{0.935 \\ 1.062 \\ 1.165 \\ \textbf{0.968}} & \makecell{1.046 \\ 1.058 \\ 1.034 \\ \textbf{0.992}} & \makecell{1.981 \\ 2.120 \\ 2.200 \\ \textbf{1.959}} \\
        \hline
        $M$ & \makecell{\textbf{2} \\ 5 \\ 10 \\ 20} & \makecell{\textbf{0.968} \\ 0.984 \\ 0.984 \\ 0.962} & \makecell{\textbf{0.992} \\ 0.998 \\ 0.997 \\ 1.015} & \makecell{\textbf{1.959} \\ 1.982 \\ 1.981 \\ 1.977}\\
        \hline
        Epochs & \makecell{100 \\ 200 \\ 500 \\ \textbf{1000}} & \makecell{0.982 \\ 0.991 \\ 0.968 \\ \textbf{0.968}} & \makecell{0.992 \\ 1.038 \\ 1.005 \\ \textbf{0.992}} & \makecell{1.974 \\ 2.029 \\ 1.972 \\ \textbf{1.959}} \\
        \hline
        Train set size & \makecell{\textbf{1} \\ 5 \\ 10 \\ 25} & \makecell{\textbf{0.968} \\ 1.067 \\ 0.995 \\ 0.983} & \makecell{\textbf{0.992} \\ 1.011 \\ 1.067 \\ 1.006} & \makecell{\textbf{1.959} \\ 2.078 \\ 2.063 \\ 1.989}\\
        \hline
    \end{tabular}
    \label{tab:params}
\end{table}

The parameters' values elected as best were the ones that provided a model that, when used for computing paths, obtained the least total cost. As for the number of epochs, the value was chosen based on cost evolution, as we can notice, from Figure \ref{fig:loss_graph}d, that after some point the test cost stops changing.

Given the data presented on the tables above, the rest of the tests were run using Softplus as the Generalized Linear Model, and the values presented on the right column of Table \ref{tab:features_parameters}.

Figure \ref{fig:loss_graph} shows statistics obtained during the training process. For measuring training and test cost evolution, path cost was calculated for computed paths at each epoch interval, both for a selected instance from the training set and for a random test instance.


\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/grad_new.png}
        \caption{Gradient of loss over time}
        \label{fig:grad_loss}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/loss_new.png}
        \caption{Loss over time}
        \label{fig:loss}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/train_new.png}
        \caption{Training cost over time}
        \label{fig:training_cost}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Images/test_new.png}
        \caption{Test cost over time}
        \label{fig:test_cost}
    \end{subfigure}
    
    \caption{Statistics of learning evolution according to the number of epochs}
    \label{fig:loss_graph}
\end{figure}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Images/perf_agents.png}
        \caption{Performance vs Number of agents}
        \label{fig:perf_agents}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Images/perf_id.png}
        \caption{Performance vs Type ID}
        \label{fig:perf_id}
    \end{subfigure}
    
    \caption{Performance on test set}
    \label{fig:perf}
\end{figure}

The information presented in Figure \ref{fig:loss_graph} shows that the final path costs for both the training and test scenarios actually increased as the model's loss decreased, indicating that, the more the model learned, the worst its impact on the final cost.

Figure \ref{fig:graphic_map} also shows the behavior of the Cost Ratio according to the number of agents
considered when testing.

Once again, it is observed that the training model's influence over the cost value tends to grow according to the number of agents considered in the instance, which is to be expected as the number of conflicts impacts directly on the training process.

To allow a more accurate visualization an analysis of the training process' output, Figure \ref{fig:heatmap} presents a heatmap of the predicted weights for a test scenario, where the black squares represent the obstacles of the map.

\begin{figure}[!ht]
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=0.925\textwidth]{Images/original_grid.png}
        \caption{Original map}
        \label{fig:original_map}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Images/heatmap_weights.png}
        \caption{Weights heatmap}
        \label{fig:heatmap}
    \end{subfigure}
    \vfill
    \caption{Weight prediction visualization from trained model}
    \label{fig:weight_pred}
\end{figure}

From the heatmap shown in Figure \ref{fig:heatmap}, it can be observed that the predicted edge weights do not follow the expected pattern. Specifically, one would reasonably expect that areas corresponding to "doors"—which typically represent traversable and frequently used paths—would have consistently higher weights compared to other regions. However, the heatmap does not reflect this behavior; high-weight values appear scattered and do not systematically align with the expected door locations. This suggests that the model may not have learned a meaningful distinction between traversable and non-traversable regions, indicating room for improvement in either the model architecture, training process, or data representation.